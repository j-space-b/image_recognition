{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5440aac0",
   "metadata": {},
   "source": [
    "# Computer Vision finetuning (using food category recognition from Food 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e491b89",
   "metadata": {},
   "source": [
    "## Summary of methodology for this use case:\n",
    "* Assuming real-time detection is needed, used R-CNN architecture with the VGG16 model as a feature extractor. \n",
    "* Additional layers such as a dense layer of 256 nodes and a dropout layer (with 0.5 rate to avoid overfitting) were incorporated\n",
    "* Final layer has 11 nodes that represent 11 unique food items in the initial dataset\n",
    "* Code runs through theoretical fine-tuning with domain-specific data on 11 categories\n",
    "* This approach enables processsing of input from a live camera input stream for results in a dashboard in near real-time with high latency, with a delay not exceeding 20 seconds\n",
    "* Included the model saved as a checkpoint at the end of the file for loading it on any future datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5202b65",
   "metadata": {},
   "source": [
    "#### Table of Contents: \n",
    "* CV model code and creation\n",
    "* Steps to load the model saved from a checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeff3f1",
   "metadata": {},
   "source": [
    "## CV model code and creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204943d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db1927f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analysis libraries \n",
    "import cv2\n",
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17dcd439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TensorFlow and Keras libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809eabb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sample dataset extraction libraries - proxy for fast food videoframes to train model \n",
    "import os \n",
    "import tarfile \n",
    "import urllib.request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "470073cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Food-101 dataset...\n",
      "Extracting the dataset...\n"
     ]
    }
   ],
   "source": [
    "# Downloading and extracting the dataset - this assumes the subdirectory 'food-101'\n",
    "URL = \"http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\"\n",
    "DATA_PATH = \"Food-101_dataset\"\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(\"Downloading Food-101 dataset...\")\n",
    "    urllib.request.urlretrieve(URL, \"food-101.tar.gz\")\n",
    "    print(\"Extracting the dataset...\")\n",
    "    with tarfile.open(\"food-101.tar.gz\", \"r:gz\") as tar:\n",
    "        tar.extractall(path=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfa715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets - this assumes the subdirectory 'food-101' if diff version throws error to adjust\n",
    "potential_path1 = os.path.join(DATA_PATH, \"food-101\", \"images\")\n",
    "potential_path2 = os.path.join(DATA_PATH, \"images\")\n",
    "\n",
    "if os.path.exists(potential_path1):\n",
    "    data_dir = potential_path1\n",
    "elif os.path.exists(potential_path2):\n",
    "    data_dir = potential_path2\n",
    "else:\n",
    "    raise ValueError(\"Could not locate the images directory in this version of the extracted dataset, check to see if diff version is hosted\")\n",
    "all_images = [] # assumes all image files are in the 'images' directory\n",
    "for subdir in os.listdir(data_dir):\n",
    "    subdir_path = os.path.join(data_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        for image_file in os.listdir(subdir_path):\n",
    "            all_images.append(os.path.join(subdir_path, image_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fdcb18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "train_data, test_data = train_test_split(all_images, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ee758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using VGG16 as a feature extractor for R-CNN\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c78945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base_model was VGG16, creating a more specific model for fine-tuning. \n",
    "# This creates some dense layers for fine-tuning.\n",
    "\n",
    "# Flatten the output of base_model\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(11, activation='softmax')(x)  # Assuming 11 classes here - will have to be the same below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c2a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the final model\n",
    "model = Model(inputs=base_model.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ab7f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine-tuning with data on the new categoreis\n",
    "# Assuming frames captured and manual labeling - if not this could be done with LLM\n",
    "# Parameter rationale: \n",
    "# The adam optimizer maintains an adaptive learning rate for each parameter and can fine-tune the model more efficiently for quicker convergence\n",
    "# The categorical crossentropy loss works well for multi-class classification, since that is the case for limited fast food items - penalizes the model more when its far off and less when it's close\n",
    "RESTAURANT_DATA_PATH = 'path_to_restaurant_data'\n",
    "if os.path.exists(RESTAURANT_DATA_PATH):\n",
    "    restaurant_data = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    train_generator = restaurant_data.flow_from_directory(RESTAURANT_DATA_PATH, target_size=(224, 224))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_generator, epochs=10)\n",
    "else:\n",
    "    print(f\"Error: Path '{RESTAURANT_DATA_PATH}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45848c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creates a dictionary to contain counts of predicted items\n",
    "def extract_items_from_predictions(predictions):\n",
    "    class_indices = np.argmax(predictions, axis=1)\n",
    "    class_counts = np.bincount(class_indices, minlength=11) \n",
    "    class_names = [\"class1\", \"class2\", \"class3\", \"class4\", \"class5\", \"class6\", \"class7\", \"class8\", \"class9\", \"class10\", \"class11\"]\n",
    "    detected_items = dict(zip(class_names, class_counts))\n",
    "    return detected_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243469a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This class displays counts of predicted items in dashboard and shows warning if latency of updates is over 20 seconds\n",
    "class Dashboard:\n",
    "    def __init__(self):\n",
    "        self.item_counts = {\n",
    "            \"class1\": 0, \"class2\": 0, \"class3\": 0, \"class4\": 0,\n",
    "            \"class5\": 0, \"class6\": 0, \"class7\": 0, \"class8\": 0,\n",
    "            \"class9\": 0, \"class10\": 0, \"class11\": 0\n",
    "        }\n",
    "\n",
    "    def update(self, detected_items):\n",
    "        start_time = time.time() # begin latency check\n",
    "        for item, count in detected_items.items():\n",
    "            self.item_counts[item] += count\n",
    "        self.display()\n",
    "        \n",
    "        elapsed_time = time.time() - start_time # end latency check\n",
    "        if elapsed_time > 20:\n",
    "            print(f)\n",
    "\n",
    "    def display(self):\n",
    "        for item, count in self.item_counts.items():\n",
    "            print(f\"Warning: Dashboard update took {elapsed_time:.2f} seconds, exceeding the 20-second threshold.\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc5fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the detection function for items\n",
    "def detect_items(frame):\n",
    "    # Preprocess frame, validate size and pass through the model\n",
    "    frame = cv2.resize(frame, (224,224))\n",
    "    processed_frame = preprocess_input(np.array([frame]))\n",
    "    predictions = model.predict(processed_frame)\n",
    "    \n",
    "    # Extract items and their bounding boxes from predictions\n",
    "    detected_items = extract_items_from_predictions(predictions)\n",
    "    \n",
    "    return detected_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faacb29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process video stream from each camera and update dashboard  \n",
    "def process_camera_stream(camera_stream, dashboard, max_frames=580608000): # max_frames limits to 40 cameras in 1 week at 24 fps, prevents any massive dumps that would break system\n",
    "    frame_count = 0\n",
    "    while frame_count < max_frames:\n",
    "        ret, frame = camera_stream.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        detected_items = detect_items(frame)\n",
    "        \n",
    "        # Update dashboard with detected items\n",
    "        dashboard.update(detected_items)\n",
    "        frame_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71f0e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize camera streams and dashboard - camera range is hardcoded based on requirements, can be changed\n",
    "camera_streams = [cv2.VideoCapture(camera_id) for camera_id in range(4*10)]\n",
    "\n",
    "dashboard = Dashboard()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14e7ba3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for camera_stream in camera_streams:\n",
    "    if camera_stream.isOpened():\n",
    "        process_camera_stream(camera_stream, dashboard)\n",
    "    else:\n",
    "        print(f\"Error: Couldn't open camera with ID {camera_streams.index(camera_stream)}\")\n",
    "\n",
    "\n",
    "    camera_stream.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b710d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the model to a checkpoint - see below for loading\n",
    "model.save('model_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcbe3f1",
   "metadata": {},
   "source": [
    "## Steps to load the model saved from a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f80c82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import load_model\n",
    "model_path = 'model_checkpoint.h5'\n",
    "loaded_model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ca3aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocess video frame data w loop to process frame by frame\n",
    "video_path = 'path_to_videoframes'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if cap.isOpened(): # check if video opened successfully \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()  # Read one frame\n",
    "        if not ret:\n",
    "            # If frame is not read properly, break the loop\n",
    "            break\n",
    "\n",
    "        # Resize the frame to fit the input size of the model - constraints of model\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "\n",
    "        # Use preprocess_input from VGG16 module\n",
    "        processed_frame = preprocess_input(np.array([frame]))  # Expanding dimensions\n",
    "\n",
    "        # Pass the processed data through the loaded model to get predictions\n",
    "        predictions = loaded_model.predict(processed_frame)\n",
    "\n",
    "        # Get the class with the highest probability\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Using the model with alignment to the 11 classes above\n",
    "        class_names = [\"class1\", \"class2\", \"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\",\"class10\",\"class11\"]  # List all your class names in order\n",
    "        print(f\"Predicted Class: {class_names[predicted_class[0]]}\")\n",
    "    \n",
    "# Release the video capture and destroy all windows if exist\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  \n",
    "else:\n",
    "    print(f\"Error: Couldn't open video '{video_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iclass2",
   "language": "python",
   "name": "iclass2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
